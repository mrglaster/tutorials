{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f677505d",
   "metadata": {},
   "source": [
    "Основной класс для ответа на вопросы и генеральной суммаризации совещания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class AnsweringQuestions():\n",
    "  '''\n",
    "  '''\n",
    "  @staticmethod\n",
    "  def answer(script_file_path, llm_name):\n",
    "    with open(script_file_path, mode='r', encoding='utf-8') as f:\n",
    "      script = json.load(f)\n",
    "      print(script.keys())\n",
    "    for file_names in script.keys(): #TS004...\n",
    "      print(file_names)\n",
    "      meeting_json = script[file_names]\n",
    "\n",
    "      meeting_transcript = ''\n",
    "      for speech in meeting_json['meeting_transcripts']:\n",
    "        meeting_transcript += f\"{speech['speaker']}: {speech['content']}\\n\"\n",
    "\n",
    "      if f'answer_{llm_name}' in meeting_json['general_query_list'][0].keys():\n",
    "        pass\n",
    "      else:\n",
    "        # general_summarization_target = meeting_json['general_query_list']\n",
    "        prompt = f\"Выполни суммаризацию в соответствии с правилами в системном запросе: {meeting_transcript}\"\n",
    "        general_summarization_prediction = summarize_with_openai(prompt=meeting_transcript, max_tokens=2000)\n",
    "        meeting_json['general_query_list'][0][f'answer_{llm_name}'] = general_summarization_prediction\n",
    "\n",
    "\n",
    "      SPECIFIC_QUERY_SYSTEM_PROMPT = \"\"\"\\\n",
    "Ты — точный и нейтральный ассистент по обработке речи. Твоя задача — создать **строго ответить на вопросы** по предоставленному тексту.\n",
    "\n",
    "**Правила:**\n",
    "1. Используй **только информацию из текста**. Ничего не выдумывай, не интерпретируй, не добавляй.\n",
    "2. Не приписывай мотивы, эмоции, цели или выводы, если они не выражены явно.\n",
    "3. Сохраняй нейтральный тон. Избегай оценочных суждений.\n",
    "4. Сфокусируйся на **существенных фактах, решениях, заявлениях, событиях**.\n",
    "5. Не используй маркированные списки, если не указано иное. Пиши связным текстом.\n",
    "6. Если текст не содержит полезной информации — напиши: \"Текст не содержит существенной информации для краткого содержания.\"\n",
    "\n",
    "Создай краткий ответ длиной от 200-300 слов\"\"\"\n",
    "\n",
    "      prompt = f'''\n",
    "Ответь на вопросы каждый на новой строчке следующим форматом длиной хотя бы в 100 слов:\n",
    "1. [ответ на первый вопрос]\n",
    "2. [ответ на второй вопрос]\n",
    "\n",
    "Отвечаешь на вопрос согласно правилам из system_prompt на следующие вопросы:\n",
    "Вопросы: '''\n",
    "\n",
    "      defined_queries = 0\n",
    "      query_index = 0\n",
    "      for index, specific_queries in enumerate(meeting_json['specific_query_list']):\n",
    "        if f'answer_{llm_name}' in meeting_json['specific_query_list'][index].keys():\n",
    "          defined_queries += 1\n",
    "          pass\n",
    "        else:\n",
    "          prompt += f\"\\n{query_index + 1}. {specific_queries['query']}\"\n",
    "          query_index += 1\n",
    "\n",
    "      prompt += f\"\\n Вот основной текст откуда берешь ответы на вопросы, тот самый текст для анализа: {meeting_transcript}\"\n",
    "\n",
    "      if defined_queries < len(meeting_json['specific_query_list']):\n",
    "        print(f'need to answer: {len(meeting_json['specific_query_list']) - defined_queries}/{len(meeting_json['specific_query_list'])}')\n",
    "        specific_query_answer = summarize_with_openai(prompt=prompt, model=llm_name, system_prompt=SPECIFIC_QUERY_SYSTEM_PROMPT, max_tokens=3000)\n",
    "        ans = specific_query_answer.split('\\n')\n",
    "        ans = [s[3:] for s in ans if s.strip()] #Dropping first 3 letters like 0. 1.\n",
    "\n",
    "        if len(ans) != len(meeting_json['specific_query_list']) - defined_queries: #defined_queries amount of defined answers so NEED to answer is ALL answers - DEFINED answers\n",
    "          print(ans)\n",
    "          raise ValueError(\"length of llm answer is not equal to amount of queries\")\n",
    "\n",
    "        ans_index = 0\n",
    "        for index, specific_query in enumerate(meeting_json['specific_query_list']):\n",
    "          if f'answer_{llm_name}' in specific_query.keys(): #Check if answer is already DEFINED, so you skip it and define the first without answer\n",
    "            pass\n",
    "            '''\n",
    "            \"query\":\n",
    "            \"answer\": <- no answer so skip\n",
    "        {\n",
    "            \"query\":\n",
    "            \"answer\":\n",
    "            \"answer_{llm_name}\": <- we are checking for that cases\n",
    "            '''\n",
    "          else:\n",
    "            specific_query[f'answer_{llm_name}'] = ans[ans_index] #Hard coded llm naming | Also ans_index is helping index only for ans because len(ans) != len(meeting_json['specific_query_list']), cuz len(ans) defined by amount of DEFINED answers but len(meeting_json['specific_query_list']) is defined only by file\n",
    "            ans_index += 1\n",
    "\n",
    "      if defined_queries == len(meeting_json['specific_query_list']):\n",
    "        print(f\"Specific queries already been answered {defined_queries}/{len(meeting_json['specific_query_list'])}\")\n",
    "\n",
    "    with open(script_file_path, mode='w', encoding='utf-8') as json_f:\n",
    "        json.dump(script, json_f, ensure_ascii=False, indent=2) #Making one big commit to main file\n",
    "    return True\n",
    "\n",
    "model_list = [\"qwen/qwen3-30b-a3b\", \"alibaba/tongyi-deepresearch-30b-a3b:free\", \"openai/gpt-oss-20b:free\", \"mistralai/mistral-small-3.2-24b-instruct:free\"]\n",
    "\n",
    "for model in model_list:\n",
    "  specific = AnsweringQuestions.answer('translated_json26102025 (1) (2).json', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ffe8f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q evaluate bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ec9d2f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# @title Чисто для суммаризации, потому что Dependency hell\n",
    "import os\n",
    "import warnings\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_api_key_envname = \"OPENAI_API_KEY\"\n",
    "SYSTEM_PROMPT = \"\"\"\\\n",
    "  Ты — точный и нейтральный ассистент по обработке речи. Твоя задача — создать **строго фактологическое краткое содержание** предоставленного текста.\n",
    "\n",
    "  **Правила:**\n",
    "  1. Используй **только информацию из текста**. Ничего не выдумывай, не интерпретируй, не добавляй.\n",
    "  2. Не приписывай мотивы, эмоции, цели или выводы, если они не выражены явно.\n",
    "  3. Сохраняй нейтральный тон. Избегай оценочных суждений.\n",
    "  4. Если в тексте несколько спикеров — укажи ключевые темы и позиции каждого (если они различаются).\n",
    "  5. Сфокусируйся на **существенных фактах, решениях, заявлениях, событиях**.\n",
    "  6. Не используй маркированные списки, если не указано иное. Пиши связным текстом.\n",
    "  7. Если текст не содержит полезной информации — напиши: \"Текст не содержит существенной информации для краткого содержания.\"\n",
    "\n",
    "  Создай краткое содержание объёмом не более 200–300 слов\"\"\"\n",
    "\n",
    "def summarize_with_openai(prompt: str = None,\n",
    "                            file_path: str = None,\n",
    "                            model: str = \"qwen/qwen3-30b-a3b\",\n",
    "                            base_url: str = \"https://openrouter.ai/api/v1\",\n",
    "                            temperature: float = 0.01,\n",
    "                            max_tokens: int = 500,\n",
    "                            system_prompt = SYSTEM_PROMPT\n",
    "                          ):\n",
    "    \"\"\"\n",
    "    Makes API call to openai services, summarizes given text\n",
    "    Args:\n",
    "        text(str): text from transcribition\n",
    "        file_path(str): path to file with transcribed text\n",
    "        model (str): Model name to use for summarization\n",
    "        base_url (str, optional): Custom base URL for OpenAI-compatible endpoints (e.g., for local LLMs)\n",
    "    Returns:\n",
    "        summarization_results(str): summarization results\n",
    "    \"\"\"\n",
    "    if file_path is None and prompt is None:\n",
    "      raise ValueError(\"Please specify file_path or paste text\")\n",
    "\n",
    "    if file_path and prompt:\n",
    "      warnings.warn(\"When text and file_path are specified in functions args, text from args(file_path) will overwrite args(text)\")\n",
    "\n",
    "    if file_path:\n",
    "      with open(file_path, mode='r') as f:\n",
    "        prompt = f.read()\n",
    "\n",
    "    open_api_key = os.getenv(openai_api_key_envname)\n",
    "    client = OpenAI(api_key=open_api_key, base_url=base_url)\n",
    "\n",
    "    print(f\"Промпт размером {len(prompt)}\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens = max_tokens\n",
    "        )\n",
    "        summary = response.choices[0].message.content.strip()\n",
    "        return summary\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error during OpenAI-compatible summarization: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474534be",
   "metadata": {},
   "source": [
    "Основной класс для проведения бенчмарка. Как пользоваться: переопределить метод summarize_with_openai чтобы ходить к своим моделям с вопросами ну и в model_list указать, указать путь к файлу с совещаниями и вопросами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed57fe5c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import warnings\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import gc\n",
    "\n",
    "model_list = [\"qwen/qwen3-30b-a3b\", \"alibaba/tongyi-deepresearch-30b-a3b:free\", \"openai/gpt-oss-20b:free\", \"mistralai/mistral-small-3.2-24b-instruct:free\"]\n",
    "\n",
    "class BenchMarking():\n",
    "  '''\n",
    "  Need to create json like that:\n",
    "  {\n",
    "    TS004: {\n",
    "      qwen30b: {\n",
    "        bertscore: {\n",
    "          general_query: 0.99\n",
    "          specific_query_avg: 0.99\n",
    "        },\n",
    "        factcc: {\n",
    "          general_query: 0.99\n",
    "          specific_query_avg: 0.99\n",
    "        }\n",
    "      },\n",
    "      qwen7b: {\n",
    "        bertscore: {\n",
    "          general_query: 0.98\n",
    "          specific_query_avg: 0.98\n",
    "        },\n",
    "        factcc: {\n",
    "          ...\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  '''\n",
    "  @staticmethod\n",
    "  def benchmark(benchmarks_results_json_path: str, scripts_json_path: str, model_list: list):\n",
    "\n",
    "\n",
    "    if benchmarks_results_json_path in os.listdir():\n",
    "      pass\n",
    "    else:\n",
    "      warnings.warn(\"Couldn't find benchmark file, creating new one\")\n",
    "      if benchmarks_results_json_path[-5:] == '.json':\n",
    "        pass\n",
    "      else:\n",
    "        benchmarks_results_json_path += '.json'\n",
    "      with open(benchmarks_results_json_path, mode='w', encoding='utf-8') as f:\n",
    "        json.dump({}, f, ensure_ascii=False)\n",
    "\n",
    "    with open(benchmarks_results_json_path, mode='r', encoding='utf-8') as f:\n",
    "      benchmark_results = json.load(f)\n",
    "    with open(scripts_json_path, mode='r', encoding='utf-8') as f:\n",
    "      script = json.load(f)\n",
    "    benchmarks_results = {}\n",
    "    for file_names in script.keys(): #TS004...\n",
    "      print(file_names)\n",
    "      benchmarks_results[file_names] = {}\n",
    "      meeting_json = script[file_names]\n",
    "      for model in model_list:\n",
    "        benchmarks_results[file_names][model] = {}\n",
    "\n",
    "        bert_results_general_query = BenchMarking.bertScore([meeting_json['general_query_list'][0][f'answer_{model}']], [meeting_json['general_query_list'][0]['answer']])['f1'][0]\n",
    "\n",
    "        targets = []\n",
    "        predictions = []\n",
    "        for specific_query in meeting_json['specific_query_list']:\n",
    "          targets.append(specific_query[f'answer'])\n",
    "          predictions.append(specific_query[f'answer_{model}'])\n",
    "        bert_results_specific_query = BenchMarking.bertScore(predictions, targets)['f1']\n",
    "        bert_results_specific_query = sum(bert_results_specific_query) / len(bert_results_specific_query)\n",
    "        benchmarks_results[file_names][model]['bertscore'] = {'general_query': bert_results_general_query, 'specific_query_avg': bert_results_specific_query}\n",
    "\n",
    "        print(bert_results_general_query, bert_results_specific_query)\n",
    "        factcc_results_general_query = json.loads(BenchMarking.LLMAsJudgeScore([meeting_json['general_query_list'][0][f'answer_{model}']], [meeting_json['general_query_list'][0]['answer']]))\n",
    "        print(factcc_results_general_query)\n",
    "        factcc_results_specific_query = {\"factuality\": 0, \"completeness\": 0, \"conciseness\": 0}\n",
    "        for specific_query in meeting_json['specific_query_list']:\n",
    "          factcc_results_interm_specific_query = json.loads(BenchMarking.LLMAsJudgeScore(specific_query[f'answer_{model}'], specific_query[f'answer']))\n",
    "          factcc_results_specific_query['factuality'] += factcc_results_interm_specific_query['factuality'] / len(meeting_json['specific_query_list'])\n",
    "          factcc_results_specific_query['completeness'] += factcc_results_interm_specific_query['completeness'] / len(meeting_json['specific_query_list'])\n",
    "          factcc_results_specific_query['conciseness'] += factcc_results_interm_specific_query['conciseness'] / len(meeting_json['specific_query_list'])\n",
    "\n",
    "        print(factcc_results_specific_query)\n",
    "        benchmarks_results[file_names][model]['llmasjudge'] = {'general_query': factcc_results_general_query, 'specific_query_avg': factcc_results_specific_query}\n",
    "\n",
    "    with open(benchmarks_results_json_path, mode='w', encoding='utf-8') as json_f:\n",
    "      json.dump(benchmarks_results, json_f, ensure_ascii=False, indent=2) #Making one big commit to main file\n",
    "\n",
    "  @staticmethod\n",
    "  def bertScore(predict, target):\n",
    "    bertscore = load(\"bertscore\")\n",
    "    results = bertscore.compute(\n",
    "        predictions=predict,\n",
    "        references=target,\n",
    "        model_type=\"bert-base-multilingual-cased\",\n",
    "        lang='ru',\n",
    "        device='cuda',\n",
    "        batch_size=8,\n",
    "    )\n",
    "\n",
    "    del bertscore\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "  @staticmethod\n",
    "  def LLMAsJudgeScore(predict, target):\n",
    "    prompt = f'''\n",
    "Ты — строгий фактолог. Твоя задача — проверить, насколько подтверждается ответ нейросети по тексту от оригинального ответа по тексту.\n",
    "\n",
    "Оцени по трём критериям (от 1 до 5):\n",
    "\n",
    "1. Фактологическая точность: насколько суммаризация соответствует исходному тексту?\n",
    "2. Полнота: охвачены ли ключевые моменты?\n",
    "3. Лаконичность: нет ли лишней информации?\n",
    "\n",
    "Оригинальный ответ по тексту: {target}\n",
    "Ответ от нейросети: {predict}\n",
    "\n",
    "Ответ в формате JSON (это всего лишь пример):\n",
    "{{\"factuality\": , \"completeness\": , \"conciseness\": }}\n",
    "ВЫВОДИ ТОЛЬКО ЭТОТ ОТВЕТ В ФОРМАТЕ JSON, НИКАКИХ ПОЯСНЕНИЙ НЕ НУЖНО\n",
    "'''\n",
    "    llm_as_judge = summarize_with_openai(prompt=prompt, model=\"qwen/qwen3-30b-a3b\", max_tokens=2000)\n",
    "    print(llm_as_judge)\n",
    "    return llm_as_judge\n",
    "\n",
    "  #   @staticmethod\n",
    "  #   def BLEUScore(predict, target):\n",
    "\n",
    "\n",
    "  # # @staticmethod\n",
    "  # def factCCScore(predict, target):\n",
    "  #   model_path = 'manueldeprada/FactCC'\n",
    "\n",
    "  #   tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "  #   model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "  #   input_dict = tokenizer(target, predict, max_length=512, padding='max_length', truncation='longest_first', return_tensors='pt')\n",
    "  #   logits = model(**input_dict).logits\n",
    "  #   # pred = logits.argmax(dim=1)\n",
    "\n",
    "  #   pred = logits.argmax(dim=1)\n",
    "  #   res = model.config.id2label[pred.item()]\n",
    "  #   score = torch.softmax(logits, dim=1)[0][pred].item()\n",
    "\n",
    "  #   return score,res\n",
    "\n",
    "BenchMarking.benchmark('bench_res.json', 'translated_json26102025 (1) (2).json', model_list=model_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b687e",
   "metadata": {},
   "source": [
    "Перевод в табличную форму"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c3f34",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('bench_res.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for meeting_id, models in data.items():\n",
    "    for model_name, metrics in models.items():\n",
    "        bs_gen = metrics['bertscore']['general_query']\n",
    "        bs_spec = metrics['bertscore']['specific_query_avg']\n",
    "\n",
    "        judge_gen = metrics['llmasjudge']['general_query']\n",
    "        judge_spec = metrics['llmasjudge']['specific_query_avg']\n",
    "\n",
    "        row = {\n",
    "            \"Meeting\": meeting_id,\n",
    "            \"Model\": model_name,\n",
    "            \"BERTScore\": f\"Gen={bs_gen:.4f}, Spec={bs_spec:.4f}\",\n",
    "            \"Factuality\": f\"Gen={judge_gen['factuality']}, Spec={judge_spec['factuality']:.2f}\",\n",
    "            \"Completeness\": f\"Gen={judge_gen['completeness']}, Spec={judge_spec['completeness']:.2f}\",\n",
    "            \"Conciseness\": f\"Gen={judge_gen['conciseness']}, Spec={judge_spec['conciseness']:.2f}\",\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df = df.sort_values(['Meeting', 'Model']).reset_index(drop=True)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
